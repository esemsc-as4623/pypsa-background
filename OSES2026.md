Delft, Netherlands
July 8 - 10

- climate science uses ensemble of models with different structures to provide robust projections and honest uncertainty characterization
- ESMs are treated as oracles
	- governments commission modelling studies to determine optimal renewable deployment pathways
	- investors use capacity expansion model outputs to justify multi-billion dollar infrastructure decisions
	- regulators cite model results when setting carbon prices or capacity market parameters
- to what extent are our conclusions artifacts of modelling choices rather than genuine insights about energy systems
- parameter uncertainty VS model uncertainty
	- approaches to parameter uncertainty = sensitivity analysis, Monte Carlo, scenario analysis
	- model uncertainty = recognizing that multiple valid framings exist and their disagreement is informative
- propose a methodology for empirically quantifying model-attributable uncertainty and distinguishing robust insights from modelling artifacts
	- technical harmonization -- parameter mappings VS post-processing adjustments to reconcile outputs
	- structural sensitivity -- which modelling choices produce economically meaningful differences in which contexts to inform model selection
	- decision-making under model uncertainty -- for a given policy question, distinguish conclusions that are robust from those that are formulation-dependent
- distinguish structural disagreement from implementation disagreement
	- structural disagreement = different models representing different plausible framings of the problem
	- implementation disagreement = different models making different implicit assumptions that could in principle be harmonized (via parameter mappings and post-processing adjustments)
- develop more rigorous practices for what depends on modelling choices we might reasonably have made differently

- sources of model divergence
	- economic
		- salvage value treatment to credit residual asset value
		- intra-period discounting to operational expenditures
		- fixed operating costs
		- depreciation method and capital recovery / allocation across investment periods
	- temporal
		- discrete decision points for lumpy investment decisions
	- supply
		- unit commitment and operational constraints
	- topological
		- transmission modelling and electrical network physics
		- reactive power flows and voltage constraints
	- policy
		- reserve margins
		- renewable energy targets
		- emission constraints and propagation of emissions prices to marginal costs

Structure:
- introduction
	- growing importance of offshore wind + storage combination for energy transition
	- investment decisions rely on capacity expansion models
	- different frameworks preferred by different stakeholders
	- RQ: do different modelling frameworks lead to different investment recommendations for offshore energy systems?
	- contribution: first systematic translation and comparison for offshore applications
	- *note: this is mainly a methodology paper using offshore case study as a representative example of how to run the comparison*
- background
	- overview of OSeMOSYS
		- LP, NPV-based cost (is it economists or policy-makers who prefer this accounting?)
		- used for coarse national/regional planning
	- overview of PyPSA
		- Python-based, object-oriented, annualized cost accounting (is it energy system operators who prefer this accounting?)
		- used in academia and industry
	- structural differences (so far)
		- cost accounting (NPV VS annualized)
		- salvage value treatment
		- discounting conventions
- methodology
	- translation framework
	- handling cost accounting differences
		- annuity conversion
		- salvage value adjustment
		- mid-year discounting
	- validation
		- same units = same physical solution (capacity, dispatch c.f. `simple.ipynb`)
		- objective function differences
- case study design
	- single offshore node (e.g. simplified North Sea hub)
	- technologies
		- offshore wind + variable capacity factor profile
		- battery storage + variable efficiency, duration
		- grid connection (fixed cost)
		- gas backup (reliability comparison)
	- time resolution
		- how many years?
		- how many sub-annual slices?
			- seasons
			- daytypes
			- timebrackets
	- demand
		- onshore demand profile (complexity depends on number of years)
	- scenarios
		- wind only
		- wind + storage
		- wind + storage + carbon price?
- results
	- compare optimal capacity recommendations
		- both models = same wind capacity?
		- both models = same storage capacity?
	- compare economic metrics
		- explainable difference in total system cost?
		- levelized cost of energy
		- storage utilization patterns
	- sensitivity to parameters
		- discount rate impact (amplified NPV VS annualized)
		- asset operational life impact (salvage value significance)
- discussion (**TBD**)
	- model choice matters less for "what to build"
	- model choices matters more for "how much it costs"
		- economic metrics diverge most significantly for long-lived assets in short planning horizons
		- what is the implication of this?
	- single node = no transmission bottleneck

---
Quantifying model-attributable uncertainty in offshore energy investment recommendations: A systematic comparison of OSeMOSYS and PyPSA

Figure ideas:
1. Translation framework schematic. A conceptual diagram illustrating the distinction between implementation disagreement and structural disagreement. The figure would show two parallel model pipelines (OSeMOSYS and PyPSA) with inputs flowing through each. Intermediate "harmonization layers" would be depicted between the pipelines, showing parameter mappings (e.g., annuity conversion, salvage value adjustment, efficiency mapping) as bidirectional arrows connecting specific model features. Structural disagreements—temporal representation, storage architecture, unit commitment—would be shown as parallel but non-connected elements, visually emphasizing that these represent genuinely different problem framings that cannot be reconciled through parameter adjustment.
2. Temporal representation comparison. A multi-panel figure showing how the same underlying wind resource data is represented in each model framework.
- **Panel A**: Full hourly wind capacity factor time series for one year (8760 points), showing characteristic North Sea variability including calm periods ("Dunkelflaute") and storm events.
- **Panel B**: PyPSA representation—either the same hourly series or a reduced representative period selection, maintaining chronological ordering.
- **Panel C**: OSeMOSYS representation—the same data aggregated into the hierarchical timeslice structure (e.g., 4 seasons × 2 day types × 4 daily brackets = 32 timeslices), showing how variability is compressed into representative values.
- **Panel D**: Overlay comparing a sample week's hourly profile versus the corresponding timeslice approximation, highlighting information loss.
- Annotations highlighting specific events (e.g., 5-day low wind period) and how they appear (or disappear) in each representation
- See the figures in https://www.sciencedirect.com/science/article/pii/S0960148117309783#fig6 and https://www.mdpi.com/1996-1073/13/3/641, which directly address the temporal aggregation challenges and trade-offs.

### Figure 1: Translation Framework Schematic

**Description**: A conceptual diagram illustrating the distinction between implementation disagreement and structural disagreement. The figure would show two parallel model pipelines (OSeMOSYS and PyPSA) with inputs flowing through each. Intermediate "harmonization layers" would be depicted between the pipelines, showing parameter mappings (e.g., annuity conversion, salvage value adjustment, efficiency mapping) as bidirectional arrows connecting specific model features. Structural disagreements—temporal representation, storage architecture, unit commitment—would be shown as parallel but non-connected elements, visually emphasizing that these represent genuinely different problem framings that cannot be reconciled through parameter adjustment.

**Design elements**:

- Left column: OSeMOSYS components (commodity-flow nodes, timeslice structure, NPV objective)
- Right column: PyPSA components (network topology, chronological snapshots, annualized objective)
- Center: Harmonization bridges for implementation differences (solid connectors) versus structural gaps (dashed lines with "≠" symbols)
- Color coding: green for harmonizable, amber for partially harmonizable, red for structural

**Reference context**: This follows the framework presentation style used by DeCarolis et al. (2017) in "Formalizing best practice for energy system optimization modelling" (_Applied Energy_), which advocates for transparency in documenting modelling assumptions and their implications. Similar conceptual schematics appear in Pfenninger et al. (2014) "Energy systems modeling for twenty-first century energy challenges" (_Renewable and Sustainable Energy Reviews_), which categorizes model features and their appropriate use cases.

---

### Figure 2: Case Study System Diagram

**Description**: A stylized schematic of the North Sea offshore energy hub showing the physical system configuration. The diagram would depict an offshore wind farm connected to a central hub node, with battery storage co-located, and a fixed-capacity HVDC cable connecting to an onshore demand node. Optional gas backup generation would be shown at the onshore node. Annotations would indicate key parameters: wind capacity factor profile source, storage duration and efficiency, transmission capacity constraint, and demand profile characteristics.

**Design elements**:

- Geographic stylization suggesting North Sea location (optional bathymetry shading)
- Component icons: wind turbines, battery, cable, gas turbine, demand/load
- Parameter callout boxes with representative values
- Arrows indicating energy flow directions

**Reference context**: This style of system diagram is standard in PyPSA-Eur documentation (Hörsch et al., 2018, "PyPSA-Eur: An open optimisation model of the European transmission system," _Energy Strategy Reviews_) and in OSeMOSYS application papers such as Niet et al. (2021) "Developing a community of practice around an open source energy modelling tool" (_Energy Strategy Reviews_). The simplified single-node representation follows the "copper plate" abstraction common in capacity expansion studies before introducing network constraints.

---

### Figure 3: Temporal Representation Comparison

**Description**: A multi-panel figure showing how the same underlying wind resource data is represented in each model framework.

- **Panel A**: Full hourly wind capacity factor time series for one year (8760 points), showing characteristic North Sea variability including calm periods ("Dunkelflaute") and storm events.
- **Panel B**: PyPSA representation—either the same hourly series or a reduced representative period selection, maintaining chronological ordering.
- **Panel C**: OSeMOSYS representation—the same data aggregated into the hierarchical timeslice structure (e.g., 4 seasons × 2 day types × 4 daily brackets = 32 timeslices), showing how variability is compressed into representative values.
- **Panel D**: Overlay comparing a sample week's hourly profile versus the corresponding timeslice approximation, highlighting information loss.

**Design elements**:

- Consistent y-axis (capacity factor 0–1) across panels
- Shading to indicate which hours map to which timeslices
- Annotations highlighting specific events (e.g., 5-day low wind period) and how they appear (or disappear) in each representation

**Reference context**: This directly addresses the temporal aggregation challenge documented by Kotzur et al. (2018) "Impact of different time series aggregation methods on optimal energy system design" (_Renewable Energy_), which systematically evaluates how representative period selection affects capacity expansion results. Similar visualizations appear in Hoffmann et al. (2020) "A Review on Time Series Aggregation Methods for Energy System Models" (_Energies_), which catalogs aggregation approaches and their trade-offs.

---

### Figure 4: Capacity Recommendation Comparison

**Description**: A grouped bar chart or diverging bar chart comparing optimal capacity recommendations from both models across the scenario progression.

- **X-axis**: Scenarios (Wind-only, Wind + 4h Battery, Wind + 8h Battery, Wind + Seasonal Storage)
- **Y-axis**: Installed capacity (GW or GWh as appropriate)
- **Grouped bars**: OSeMOSYS (one color) versus PyPSA (another color) for each technology
- **Sub-panels or facets**: Separate for wind capacity, storage power capacity, storage energy capacity

An alternative "parity plot" format could show PyPSA capacity on x-axis versus OSeMOSYS capacity on y-axis, with a 1:1 line indicating perfect agreement. Points near the line indicate robust recommendations; points far from the line indicate model-dependent conclusions.

**Design elements**:

- Error bars or ranges if sensitivity analysis is included
- Annotation of percentage divergence for each comparison
- Visual threshold indicating "economically meaningful" divergence (e.g., ±10%)

**Reference context**: This comparative format follows the multi-model comparison studies in the Energy Modeling Forum (EMF) tradition, such as Bistline et al. (2023) "Emissions and energy impacts of the Inflation Reduction Act" (_Science_), which presents capacity projections across multiple models. The parity plot approach is used by Trutnevyte (2016) "Does cost optimization approximate the real-world energy transition?" (_Applied Energy_) to compare model outputs against observed deployment.

---

### Figure 5: Economic Divergence Analysis

**Description**: A multi-panel figure examining divergence in economic metrics.

- **Panel A**: Total system cost comparison (bar chart) showing OSeMOSYS versus PyPSA for each scenario, with costs decomposed into capital, fixed O&M, variable O&M, and salvage value credit. This reveals which cost components drive divergence.
- **Panel B**: Levelized cost of energy (LCOE) or levelized cost of storage (LCOS) comparison, showing how different cost accounting conventions affect commonly-reported metrics.
- **Panel C**: Scatter plot of capacity divergence (x-axis) versus cost divergence (y-axis) for all scenarios and technologies, testing the hypothesis that costs diverge more than capacities.

**Design elements**:

- Stacked bar decomposition to show salvage value as negative (credit) component
- Annotations indicating percentage of divergence attributable to each cost component
- Reference lines in Panel C indicating where cost divergence exceeds capacity divergence

**Reference context**: Cost decomposition approaches follow Ueckerdt et al. (2013) "System LCOE: What are the costs of variable renewables?" (_Energy_), which emphasizes that cost metrics depend on accounting conventions and system boundaries. The analysis of salvage value significance relates to Loulou et al. (2016) documentation of the TIMES model, which discusses alternative depreciation and capital recovery treatments and their implications for long-lived assets.

---

### Figure 6: Sensitivity to Planning Horizon and Discount Rate

**Description**: A heatmap or contour plot showing how model divergence varies across a two-dimensional parameter space.

- **X-axis**: Planning horizon length (e.g., 10, 15, 20, 25, 30 years)
- **Y-axis**: Discount rate (e.g., 3%, 5%, 7%, 10%)
- **Color intensity**: Magnitude of divergence in total system cost (or capacity, as separate panel)
- **Contour lines**: Iso-divergence curves identifying combinations where models agree within specified tolerance

A complementary panel could show divergence specifically in salvage value as a fraction of total cost, illustrating why longer asset lives relative to shorter planning horizons amplify economic divergence.

**Design elements**:

- Annotations at specific parameter combinations used in main scenarios
- Marginal histograms showing distribution of divergence along each axis
- Threshold shading indicating "acceptable" versus "significant" divergence regions

**Reference context**: This sensitivity analysis format is common in energy economics, following the approach of Hirth (2013) "The market value of variable renewables" (_Energy Economics_), which uses parameter sweeps to identify where conclusions are robust versus parameter-dependent. The focus on discount rate sensitivity follows Emmerling et al. (2019) "The role of the discount rate for emission pathways and negative emissions" (_Environmental Research Letters_).

---

### Figure 7: Curtailment and Storage Utilization Patterns

**Description**: A detailed operational comparison for the wind + storage scenario.

- **Panel A**: Duration curves of curtailment—hours sorted by curtailment magnitude for both models, revealing whether OSeMOSYS's timeslice structure underestimates peak curtailment events while potentially matching total annual curtailment.
- **Panel B**: Storage state-of-charge trajectories for a representative period (e.g., one month)—PyPSA's hourly trajectory versus OSeMOSYS's stepped representation across timeslice transitions.
- **Panel C**: Storage cycling frequency distribution—histogram of charge/discharge cycles by depth-of-discharge, showing whether models predict similar utilization patterns.

**Design elements**:

- Shaded area between curves indicating divergence
- Annotations for specific events (e.g., multi-day storage depletion during low-wind period)
- Summary statistics (total curtailment TWh, equivalent full cycles) in legend

**Reference context**: Duration curve analysis for curtailment follows Denholm and Hand (2011) "Grid flexibility and storage required to achieve very high penetration of variable renewable electricity" (_Energy Policy_). Storage dispatch visualization approaches appear in Sepulveda et al. (2021) "The design space for long-duration energy storage in decarbonized power systems" (_Nature Energy_), which examines how storage operational patterns depend on temporal resolution and system configuration.

---

### Figure 8: Robust Versus Formulation-Dependent Conclusions

**Description**: A summary figure presenting the key epistemic contribution—distinguishing robust insights from modelling artifacts.

- **Format**: A "traffic light" matrix or classification table
- **Rows**: Key conclusions (e.g., "Optimal wind capacity," "Optimal short-duration storage capacity," "Optimal long-duration storage capacity," "Total system cost," "LCOE," "Curtailment rate")
- **Columns**: Agreement status after harmonization
- **Cells**: Green (robust—models agree within 10%), Amber (sensitive—agreement depends on harmonization choices), Red (structural—models diverge due to fundamental framing differences)

An alternative "ensemble spread" visualization could show each conclusion as a point with error bars, where the error bar represents model spread—tight bars indicate robust conclusions, wide bars indicate model uncertainty.

**Design elements**:

- Clear visual hierarchy emphasizing the robust/sensitive/structural classification
- Footnotes explaining which harmonization steps were applied
- Call-out boxes linking specific divergences to their structural causes

**Reference context**: This ensemble interpretation approach draws from climate science practice, as advocated by Tebaldi and Knutti (2007) "The use of the multi-model ensemble in probabilistic climate projections" (_Philosophical Transactions of the Royal Society A_). In energy modelling, this perspective is articulated by DeCarolis et al. (2017) and by Yue et al. (2018) "A review of approaches to uncertainty assessment in energy system optimization models" (_Energy Strategy Reviews_), which calls for explicit treatment of model structural uncertainty alongside parameter uncertainty.

---

### Figure 9 (Optional): Translation Validation

**Description**: A diagnostic figure demonstrating that the translation framework correctly harmonizes implementation differences before structural comparisons are made.

- **Panel A**: Simple test case (e.g., single technology, single year) showing that both models produce identical capacity and dispatch when structural differences are neutralized (e.g., PyPSA using equivalent timeslice weights, storage disabled).
- **Panel B**: Objective function decomposition showing that remaining cost differences after harmonization are attributable to documented structural features, not translation errors.

**Design elements**:

- Near-perfect overlay of dispatch profiles for validation case
- Checklist of harmonization steps with verification status
- Residual difference quantification (should be <1% for validated translation)

**Reference context**: This validation approach follows best practices outlined in DeCarolis et al. (2017) and implemented in model intercomparison projects. Similar validation is presented in Huppmann et al. (2019) "The MESSAGEix Integrated Assessment Model and the ix modeling platform" (_Environmental Modelling & Software_), which documents how model translations are verified for consistency.

