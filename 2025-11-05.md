#benchmark

Energy system catapult
- join a mailing lists
- they have their own model?

draw up a list of key people in the area
- who might be a reviewer for our paper?

benchmark comparison
- set up and run OSeMOSYS to compare with PyPSA
- where did these models come from?
- choose models that were built structurally in a different way
- automated translation of input files?
- when do the results start to diverge?
- what are the reasons for divergence?
	- bug (accumulate or average out?)
- code surgery, turning on/off some coupling?

consider talking to users / knowledgeable people
- can you set up the parameters and run?
- hierarchy of simple models that grow in complexity

Danny P. (Imperial) closed source model

Ellyess will graduate in a few months
- work alongside to help his final paper published
	- effect of wakes
	- how representation manifests in output of ESMs
- how do these models represent physics and resolution
	- solution verification
- benchmarking = code verification
- North Sea (UK ++)

Yan (another PhD student)
- bias correction in wind fields that people use to represent the resource
- data for all of Europe
- how impactful is the reduction of errors in the climate model by 30%
- global scale, coarse (reanalysis = forecast + historical)
	- does not account for topography
	- compared with actual turbine generation data
	- correct wind field with a bias correction model
	- there is also a simply corrected wind field

Matt is interested in identifying flaws and doing things better
- code verification = bugs in code
- solution verification = choices that a user makes (e.g. representation, resolution)
- validation = implementation / design choices (e.g. absence of wakes)
- rigorous experimentation changes one thing and attributes differences to it
- do not conflate multiple changes

PyPSA
- how are wakes from wind farms currently represented?

MUSE = ModUlar energy system Simulation Environment
- agent-based approach
- https://www.imperial.ac.uk/muse-energy/what-is-muse-/

